{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте поставим эксперимент и проверим, так ли хороша на самом деле эта нормализованная инициализация. Для этого зададим простую полносвязную модель и будем оценивать точность на тестовом множестве в датасете MNIST. Сначала импортируем все необходимое из Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Art\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers import Dense"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Как и в TensorFlow, в Keras набор данных MNIST доступен «из коробки»:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://s3.amazonaws.com/img-datasets/mnist.npz\n",
      "11493376/11490434 [==============================] - 26s 2us/step\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "(x_train, y_train), (x_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Однако на этот раз правильные ответы заданы в виде цифр, и нам придется самостоятельно перекодировать их в виде векторов. Для этого можно использовать модуль np_utils, входящий в состав Keras:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import np_utils\n",
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь осталось для удобства перевести матрицы X_train и X_test из целочисленных значений на отрезке [0,255] к вещественным на [0,1] (нормализовать), а также сделать из квадратных изображений размера 28 х 28 пикселов одномерные векторы длины 784; это значит, что сами тензоры X_train и X_test будут иметь размерность (число примеров) х 784:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.reshape([-1, 28*28]) / 255.\n",
    "x_test = x_test.reshape([-1, 28*28]) / 255."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Все, данные готовы. Поскольку мы собираемся определять сразу две одинаковые модели, различающиеся только способом инициализации весов, давайте сразу объявим соответствующую функцию."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(init):\n",
    "    model = Sequential()\n",
    "    model.add(Dense(100, input_shape=(28*28,), activation='tanh', kernel_initializer=init))\n",
    "    model.add(Dense(100, activation='tanh', kernel_initializer=init))\n",
    "    model.add(Dense(100, activation='tanh', kernel_initializer=init))\n",
    "    model.add(Dense(100, activation='tanh', kernel_initializer=init))\n",
    "    model.add(Dense(10, activation='softmax', kernel_initializer=init))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом коде функция create_model принимает на вход текстовый параметр, который интерпретируется как тип инициализации. Для нашего эксперимента это будут значения uniform и glorot_normal. А возвращает функция простую полносвязную модель с четырьмя промежуточными слоями, каждый размера 100. Везде, кроме последнего слоя, мы использум в качестве функции активации гиперболический тангенс, а в последнем слое — softmax, так как собираемся использовать в качестве функции потерь перекрестную энтропию.  \n",
    "Процесс компиляции модели и ее обучения задается очень просто:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 2.3010 - acc: 0.1118 - val_loss: 2.2991 - val_acc: 0.1135\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 12s 208us/step - loss: 2.2970 - acc: 0.1124 - val_loss: 2.2935 - val_acc: 0.1135\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 11s 185us/step - loss: 2.2795 - acc: 0.1693 - val_loss: 2.2333 - val_acc: 0.2624\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 1.8645 - acc: 0.2908 - val_loss: 1.4092 - val_acc: 0.4201\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 1.2485 - acc: 0.5189 - val_loss: 1.1231 - val_acc: 0.6213\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 12s 197us/step - loss: 0.9634 - acc: 0.6840 - val_loss: 0.7922 - val_acc: 0.7473\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.7194 - acc: 0.7722 - val_loss: 0.6446 - val_acc: 0.8081\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.5916 - acc: 0.8273 - val_loss: 0.5240 - val_acc: 0.8543\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 12s 192us/step - loss: 0.4928 - acc: 0.8620 - val_loss: 0.4426 - val_acc: 0.8803\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 12s 207us/step - loss: 0.4181 - acc: 0.8847 - val_loss: 0.3814 - val_acc: 0.8993\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 11s 184us/step - loss: 0.3628 - acc: 0.9006 - val_loss: 0.3584 - val_acc: 0.9000\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.3223 - acc: 0.9113 - val_loss: 0.3262 - val_acc: 0.9088\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.2885 - acc: 0.9204 - val_loss: 0.2846 - val_acc: 0.9219\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.2597 - acc: 0.9281 - val_loss: 0.2679 - val_acc: 0.9259\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 11s 188us/step - loss: 0.2347 - acc: 0.9346 - val_loss: 0.2439 - val_acc: 0.9332\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.2135 - acc: 0.9407 - val_loss: 0.2156 - val_acc: 0.9411\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.1960 - acc: 0.9455 - val_loss: 0.2143 - val_acc: 0.9409\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.1799 - acc: 0.9494 - val_loss: 0.1946 - val_acc: 0.9449\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.1677 - acc: 0.9534 - val_loss: 0.2229 - val_acc: 0.9346\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 11s 192us/step - loss: 0.1563 - acc: 0.9556 - val_loss: 0.1740 - val_acc: 0.9494\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 11s 192us/step - loss: 0.1468 - acc: 0.9584 - val_loss: 0.1649 - val_acc: 0.9524\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 11s 187us/step - loss: 0.1382 - acc: 0.9607 - val_loss: 0.1662 - val_acc: 0.9526\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 11s 189us/step - loss: 0.1307 - acc: 0.9627 - val_loss: 0.1582 - val_acc: 0.9544\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.1239 - acc: 0.9644 - val_loss: 0.1614 - val_acc: 0.9534\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.1173 - acc: 0.9665 - val_loss: 0.1650 - val_acc: 0.9527\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.1119 - acc: 0.9684 - val_loss: 0.1535 - val_acc: 0.9547\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 12s 196us/step - loss: 0.1067 - acc: 0.9698 - val_loss: 0.1508 - val_acc: 0.9572\n",
      "Epoch 28/30\n",
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.1022 - acc: 0.9716 - val_loss: 0.1486 - val_acc: 0.9575\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 12s 193us/step - loss: 0.0979 - acc: 0.9726 - val_loss: 0.1383 - val_acc: 0.9614\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 11s 186us/step - loss: 0.0939 - acc: 0.9734 - val_loss: 0.1376 - val_acc: 0.9607\n",
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/30\n",
      "60000/60000 [==============================] - 12s 208us/step - loss: 0.7258 - acc: 0.8143 - val_loss: 0.3842 - val_acc: 0.8993\n",
      "Epoch 2/30\n",
      "60000/60000 [==============================] - 12s 195us/step - loss: 0.3515 - acc: 0.9028 - val_loss: 0.3002 - val_acc: 0.9172\n",
      "Epoch 3/30\n",
      "60000/60000 [==============================] - 12s 201us/step - loss: 0.2938 - acc: 0.9167 - val_loss: 0.2627 - val_acc: 0.9241\n",
      "Epoch 4/30\n",
      "60000/60000 [==============================] - 12s 199us/step - loss: 0.2599 - acc: 0.9250 - val_loss: 0.2389 - val_acc: 0.9321\n",
      "Epoch 5/30\n",
      "60000/60000 [==============================] - 15s 243us/step - loss: 0.2343 - acc: 0.9322 - val_loss: 0.2196 - val_acc: 0.9353 1s - lo\n",
      "Epoch 6/30\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 0.2136 - acc: 0.9390 - val_loss: 0.2035 - val_acc: 0.9416\n",
      "Epoch 7/30\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.1958 - acc: 0.9431 - val_loss: 0.1862 - val_acc: 0.9459\n",
      "Epoch 8/30\n",
      "60000/60000 [==============================] - 14s 230us/step - loss: 0.1804 - acc: 0.9480 - val_loss: 0.1760 - val_acc: 0.9488\n",
      "Epoch 9/30\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.1671 - acc: 0.9518 - val_loss: 0.1644 - val_acc: 0.9508\n",
      "Epoch 10/30\n",
      "60000/60000 [==============================] - 13s 219us/step - loss: 0.1558 - acc: 0.9553 - val_loss: 0.1541 - val_acc: 0.9539\n",
      "Epoch 11/30\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.1456 - acc: 0.9577 - val_loss: 0.1474 - val_acc: 0.9554\n",
      "Epoch 12/30\n",
      "60000/60000 [==============================] - 13s 222us/step - loss: 0.1365 - acc: 0.9605 - val_loss: 0.1437 - val_acc: 0.957167 -\n",
      "Epoch 13/30\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.1282 - acc: 0.9627 - val_loss: 0.1333 - val_acc: 0.9585\n",
      "Epoch 14/30\n",
      "60000/60000 [==============================] - 13s 225us/step - loss: 0.1210 - acc: 0.9649 - val_loss: 0.1318 - val_acc: 0.9611\n",
      "Epoch 15/30\n",
      "60000/60000 [==============================] - 14s 227us/step - loss: 0.1144 - acc: 0.9671 - val_loss: 0.1252 - val_acc: 0.9611\n",
      "Epoch 16/30\n",
      "60000/60000 [==============================] - 14s 229us/step - loss: 0.1084 - acc: 0.9686 - val_loss: 0.1217 - val_acc: 0.9628\n",
      "Epoch 17/30\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.1029 - acc: 0.9705 - val_loss: 0.1174 - val_acc: 0.9643\n",
      "Epoch 18/30\n",
      "60000/60000 [==============================] - 15s 246us/step - loss: 0.0979 - acc: 0.9717 - val_loss: 0.1114 - val_acc: 0.9664\n",
      "Epoch 19/30\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0930 - acc: 0.9731 - val_loss: 0.1098 - val_acc: 0.9671\n",
      "Epoch 20/30\n",
      "60000/60000 [==============================] - 14s 232us/step - loss: 0.0886 - acc: 0.9740 - val_loss: 0.1130 - val_acc: 0.9648\n",
      "Epoch 21/30\n",
      "60000/60000 [==============================] - 14s 238us/step - loss: 0.0846 - acc: 0.9755 - val_loss: 0.1061 - val_acc: 0.9676\n",
      "Epoch 22/30\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0808 - acc: 0.9764 - val_loss: 0.1027 - val_acc: 0.9673\n",
      "Epoch 23/30\n",
      "60000/60000 [==============================] - 14s 235us/step - loss: 0.0774 - acc: 0.9777 - val_loss: 0.1020 - val_acc: 0.9682\n",
      "Epoch 24/30\n",
      "60000/60000 [==============================] - 15s 249us/step - loss: 0.0739 - acc: 0.9785 - val_loss: 0.0981 - val_acc: 0.9688\n",
      "Epoch 25/30\n",
      "60000/60000 [==============================] - 14s 239us/step - loss: 0.0708 - acc: 0.9792 - val_loss: 0.0969 - val_acc: 0.9691\n",
      "Epoch 26/30\n",
      "60000/60000 [==============================] - 15s 251us/step - loss: 0.0678 - acc: 0.9807 - val_loss: 0.0953 - val_acc: 0.9698\n",
      "Epoch 27/30\n",
      "60000/60000 [==============================] - 15s 255us/step - loss: 0.0651 - acc: 0.9812 - val_loss: 0.0967 - val_acc: 0.9693\n",
      "Epoch 28/30\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60000/60000 [==============================] - 11s 190us/step - loss: 0.0623 - acc: 0.9822 - val_loss: 0.0930 - val_acc: 0.9701\n",
      "Epoch 29/30\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0599 - acc: 0.9829 - val_loss: 0.0903 - val_acc: 0.9713\n",
      "Epoch 30/30\n",
      "60000/60000 [==============================] - 12s 194us/step - loss: 0.0573 - acc: 0.9838 - val_loss: 0.0924 - val_acc: 0.9702\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2a40c45ccf8>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "uniform_model = create_model(\"uniform\")\n",
    "uniform_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "uniform_model.fit(x_train, Y_train, \n",
    "                  batch_size=64, epochs=30, verbose=1, validation_data=(x_test, Y_test))\n",
    "\n",
    "glorot_model = create_model(\"glorot_normal\")\n",
    "glorot_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "glorot_model.fit(x_train, Y_train, \n",
    "                  batch_size=64, epochs=30, verbose=1, validation_data=(x_test, Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
