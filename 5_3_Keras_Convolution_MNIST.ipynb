{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А теперь давайте для сравнения сделаем то же самое, но в Keras. Суть процесса, конечно, не меняется, но кода получается заметно меньше. Как и в TensorFlow, в Keras набор данных MNIST можно загрузить средствами самой библиотеки:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Затем давайте подготовим данные к тому, чтобы подать их на вход сети. Здесь нам потребуются три вспомогательные процедуры. Во-первых, нужно будет, как и раньше, превратить каждую картинку в двумерный массив:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size, img_rows, img_cols = 64, 28, 28\n",
    "X_train = X_train.reshape(X_train.shape[0], img_rows, img_cols, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], img_rows, img_cols, 1)\n",
    "input_shape = (img_rows, img_cols, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Во-вторых, входные данные MNIST в Keras, в отличие от TensorFlow, представляют собой целые числа от 0 до 255; можно было бы обучать сеть и на таких данных, но давайте для единообразия все-таки приведем их к типу flo a t3 2 и нормализуем от 0 до 1, как раньше:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = X_train.astype(\"float32\")\n",
    "X_test = X_test.astype(\"float32\")\n",
    "X_train /= 255\n",
    "X_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В-третьих, переведем правильные ответы в one-hot представление; для этого служит вспомогательная процедура to_categorical из keras.np_utils:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Y_train = np_utils.to_categorical(y_train, 10)\n",
    "Y_test = np_utils.to_categorical(y_test, 10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь пора задавать собственно модель. В Keras это выглядит примерно так же, как в TensorFlow, но некоторые названия и параметры более узнаваемы. Сначала инициализируем модель:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь добавим сверточные слои. Нам понадобится слой Convolution2D, основными аргументами которого являются число фильтров и размер окна, аргумент border_mode имеет ровно тот же смысл, что аргумент padding в TensorFlow (а теперь и в Keras это padding), а аргумент input_shape сообщает Keras, какой размерности тензор ожидать на входе:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Conv2D(filters=32, kernel_size=(5, 5), strides=(5, 5), \n",
    "                 padding=\"same\", input_shape=input_shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\"))\n",
    "model.add(Conv2D(filters=64, kernel_size=(5, 5), strides=(5, 5), \n",
    "                 padding=\"same\", input_shape=input_shape))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2), strides=(2, 2), padding=\"same\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В этом примере мы сохранили ту же архитектуру, что была выше. Обратите внимание, что в Keras нам не нужно отдельно инициализировать переменные, которые будут затем использоваться в качестве весов или свободных членов в слоях: Keras сам понимает, сколько должно быть весов у той или иной сверточной архитектуры, и сам поймет, что по ним нужно модель оптимизировать. Единственное, чем ему нужно для этого помочь, — задать явно размерность input_shape.  \n",
    "\n",
    "Полносвязные слои тоже получаются достаточно просто. За «переформатирование» тензора, которое в TensorFlow делалось функцией tf.reshape, теперь отвечает дополнительный «слой», который называется Flatten и способен сам понять, тензор какой формы подается ему на вход:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.add(Flatten())\n",
    "model.add(Dense(1024))\n",
    "model.add(Activation(\"relu\"))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(10))\n",
    "model.add(Activation(\"softmax\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И все, можно компилировать и запускать обучение:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 33s 545us/step - loss: 0.0810 - acc: 0.9731 - val_loss: 0.1222 - val_acc: 0.9622\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 31s 516us/step - loss: 0.0733 - acc: 0.9752 - val_loss: 0.1153 - val_acc: 0.9669\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 32s 531us/step - loss: 0.0702 - acc: 0.9765 - val_loss: 0.1146 - val_acc: 0.9659\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 33s 549us/step - loss: 0.0631 - acc: 0.9784 - val_loss: 0.1190 - val_acc: 0.9677\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 32s 537us/step - loss: 0.0610 - acc: 0.9789 - val_loss: 0.1237 - val_acc: 0.9650\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 34s 564us/step - loss: 0.0557 - acc: 0.9813 - val_loss: 0.1262 - val_acc: 0.9648 ac\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 43s 719us/step - loss: 0.0554 - acc: 0.9813 - val_loss: 0.1208 - val_acc: 0.9676\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 35s 581us/step - loss: 0.0521 - acc: 0.9822 - val_loss: 0.1143 - val_acc: 0.9695\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 36s 597us/step - loss: 0.0493 - acc: 0.9830 - val_loss: 0.1358 - val_acc: 0.9638\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 37s 613us/step - loss: 0.0462 - acc: 0.9841 - val_loss: 0.1307 - val_acc: 0.9678\n",
      "Test score: 0.130706\n",
      "Test accuracy: 0.967800\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "model.fit(X_train, Y_train, batch_size=batch_size, epochs=10, verbose=1, \n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test score: %f\" % score[0])\n",
    "print(\"Test accuracy: %f\" % score[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Чтобы избежать оверфиттинга, можно, конечно, написать цикл по эпохам обучения, проверять в этом цикле\n",
    "ошибку на валидационном множестве, сохранять веса модели в файл, а затем выбрать файл, соответствующий самой лучшей эпохе. Но оказывается, что в Keras все это можно выразить буквально в аргументах функции fit! Вот как обычно выглядит запуск обучения сложных моделей на практике:   \n",
    "\n",
    "Здесь аргумент callbacks позволяет задать функции, запускаемые после каждой эпохи обучения. Можно написать эти функции самому, а можно воспользоваться стандартными. Так, функция ModelCheckpoint из модуля keras.callbacks — это вспомогательная процедура, которая после каждой эпохи обучения сохраняет модель в файл, имя которого подается на вход, в данном случае model.hdf5. А параметр save_best_only=True позволяет после каждой эпохи проверять метрику качества, заданную в параметре monitor (в данном случае val_acc, то есть точность на валидационном множестве), и перезаписывать сохраняемую модель только в том случае, если эта метрика улучшилась. Обратите также внимание на параметр valldation_split=0.1: если у вас нет заранее выделенного тренировочного и тестового подмножеств (у нас они были сразу в датасете MNIST), то можно просто попросить Keras использовать для валидации случайное подмножество входных данных, составляющее заданную их долю, в данном случае 10 %."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 60000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "60000/60000 [==============================] - 34s 566us/step - loss: 0.0441 - acc: 0.9845 - val_loss: 0.1298 - val_acc: 0.9664\n",
      "Epoch 2/10\n",
      "60000/60000 [==============================] - 34s 565us/step - loss: 0.0428 - acc: 0.9856 - val_loss: 0.1341 - val_acc: 0.9679\n",
      "Epoch 3/10\n",
      "60000/60000 [==============================] - 38s 626us/step - loss: 0.0404 - acc: 0.9862 - val_loss: 0.1440 - val_acc: 0.9669\n",
      "Epoch 4/10\n",
      "60000/60000 [==============================] - 32s 538us/step - loss: 0.0379 - acc: 0.9862 - val_loss: 0.1422 - val_acc: 0.9674\n",
      "Epoch 5/10\n",
      "60000/60000 [==============================] - 36s 598us/step - loss: 0.0352 - acc: 0.9879 - val_loss: 0.1485 - val_acc: 0.9669\n",
      "Epoch 6/10\n",
      "60000/60000 [==============================] - 34s 568us/step - loss: 0.0350 - acc: 0.9879 - val_loss: 0.1468 - val_acc: 0.9679\n",
      "Epoch 7/10\n",
      "60000/60000 [==============================] - 36s 603us/step - loss: 0.0363 - acc: 0.9875 - val_loss: 0.1436 - val_acc: 0.9671\n",
      "Epoch 8/10\n",
      "60000/60000 [==============================] - 31s 515us/step - loss: 0.0325 - acc: 0.9884 - val_loss: 0.1572 - val_acc: 0.9656\n",
      "Epoch 9/10\n",
      "60000/60000 [==============================] - 32s 539us/step - loss: 0.0364 - acc: 0.9875 - val_loss: 0.1602 - val_acc: 0.9652s - loss: 0.0366 - acc: 0. - ETA: 1s - lo - ETA: 0s - loss: 0.0362 - acc: 0\n",
      "Epoch 10/10\n",
      "60000/60000 [==============================] - 33s 549us/step - loss: 0.0299 - acc: 0.9897 - val_loss: 0.1542 - val_acc: 0.9673\n",
      "Test score: 0.154222\n",
      "Test accuracy: 0.967300\n"
     ]
    }
   ],
   "source": [
    "from keras.callbacks import ModelCheckpoint\n",
    "model.fit(X_train, Y_train, \n",
    "          callbacks=[ModelCheckpoint(\"model.hdf5\", monitor=\"val_acc\", save_best_only=True,\n",
    "          save_weights_only=False, mode=\"auto\")], batch_size=batch_size, epochs=10, verbose=1, \n",
    "          validation_data=(X_test, Y_test))\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "print(\"Test score: %f\" % score[0])\n",
    "print(\"Test accuracy: %f\" % score[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
