{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку MNIST — это своего рода Hello World для современной обработки\n",
    "изображений, в TensorFlow этот набор данных поддерживается «из коробки»,\n",
    "и для импорта MNIST достаточно написать буквально две строчки кода:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Art\\Anaconda3\\lib\\site-packages\\h5py\\__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-1-9985e2cc5995>:2: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From C:\\Users\\Art\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From C:\\Users\\Art\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Art\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Art\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From C:\\Users\\Art\\Anaconda3\\lib\\site-packages\\tensorflow\\contrib\\learn\\python\\learn\\datasets\\mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data \n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Полученные данные уже разбиты на тренировочную (mnist.train), тестовую\n",
    "(mnist.test) и валидационную (mnist.validate) выборки, содержащие 55 000, 10 000\n",
    "и 5000 примеров соответственно. Каждая из этих выборок состоит из изображений\n",
    "цифр (mnist.train.images) и их меток (mnist.train.labels)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В TensorFlow требуемые операции выражаются с помощью символьных переменных,\n",
    "поэтому давайте создадим переменную для тренировочных данных:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = tf.placeholder(tf.float32, [None, 784])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "В данном случае х — это не какой-то заранее заданный тензор, а так называемая\n",
    "заглушка (placeholder), которую мы заполним, когда попросим TensorFlow произвести\n",
    "вычисления. Мы хотим иметь возможность использовать произвольное число\n",
    "784-мерных векторов для обучения, поэтому в качестве одной из размерностей\n",
    "указываем None. Для TensorFlow это значит, что данная размерность может иметь\n",
    "произвольную длину.  \n",
    "Кроме заглушки для тренировочных данных, нам также потребуются переменные,\n",
    "которые мы собственно и будем изменять при обучении нашей модели."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([784, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Здесь W имеет размерность 784 x 10, так как мы собираемся умножать вектор\n",
    "размерности 784 на W и получать предсказание для 10 возможных меток, а вектор b\n",
    "размерности 10 — это свободный член, bias, который мы добавляем к выходу.  \n",
    "После того как мы импортировали нужные модули и объявили все переменные,\n",
    "собственно нашу модель на TensorFlow можно записать в одну строчку!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = tf.nn.softmax(tf.matmul(x, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сначала мы перемножаем матрицы х и W с помощью tf.matmul(x,W), затем добавляем к результату Ь, и для получения вероятностей классов применяем tf.nn.softnax.   \n",
    "Для того чтобы обучить модель, нужно также зафиксировать некий способ оценки качества предсказаний (именно эту оценку мы и будем в конечном счете оптимизировать). Опишем теперь в терминах TensorFlow функцию потерь. Для исходной разметки нам понадобится заглушка:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_ = tf.placeholder(tf.float32, [None, 10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь функцию потерь тоже можно записать в одну строчку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(-tf.reduce_sum(y_ * tf.log(y), reduction_indices=[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте более подробно разберем эту строку. Посмотрим, что будет происходить, последовательно, от внутренних операций к внешним:\n",
    "- сначала мы вычисляем tf.log(y), логарифм каждого элемента у;\n",
    "- затем умножаем каждый элемент у_ на соответствующий ему tf.log(y) (операция умножения на векторах, матрицах и тензорах здесь понимается покомпонентно);\n",
    "- затем суммируем результат с помощью tf.reduce_sum по второму измерению; напомним, что первое измерение — это примеры из тестового или валидационного множества, а второе — возможные классы, то есть мы суммируем не по примерам, а по размерности каждого вектора у; для этого мы указали в качестве параметра reductlon_lndlces=[1];\n",
    "- и наконец, последняя операция tf.reduce_mean подсчитывает среднее значение по всем примерам в выборке."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь, когда мы знаем, чего мы хотим от нашей модели, мы можем попросить TensorFlow оптимизировать эту функцию. Да, это именно настолько просто. Мы уже полностью описали граф вычислений в терминах, понятных TensorFlow, все вершины в этом графе содержат известные классические функции, градиенты которых, разумеется, уже реализованы в TensorFlow, и библиотека может воспользоваться алгоритмом обратного распространения ошибки для того, чтобы подсчитать градиенты, узнать, как веса влияют на функцию потерь, которую мы хотим минимизировать, и затем применить тот или иной алгоритм оптимизации для уточнения этих весов.  \n",
    "Для обучения модели мы будем использовать метод градиентного спуска со скоростью обучения 0,5:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_step = tf.train.GradientDescentOptimizer(0.5).minimize(cross_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Перед началом обучения осталось инициализировать переменные:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы готовы запускать обучение. Для этого создаем TensorFlow-ceccию..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "sess = tf.Session()\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...и пора запускать:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(1000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На каждом проходе цикла мы выбираем случайные 100 примеров из обучающей выборки и передаем их в функцию tra in _ s te p для обучения. Такой подход мы уже обсуждали — это типичный стохастический градиентный спуск; в данном случае мы применяем его потому, что обучающая выборка слишком велика для\n",
    "того, чтобы проходить по ней целиком на каждом шаге обучения (собственно, так будет всегда на любых данных хоть сколько-нибудь реального размера). Вместо этого мы выбираем каждый раз небольшой новый случайный набор обучающих данных и используем его.  \n",
    "Осталось понять, насколько хорошо мы теперь умеем распознавать рукописные цифры. Модель на данный момент выдает предсказания в виде softmax-результатов — суммирующихся в единицу чисел, отражающих уверенность модели в том или ином ответе. Для того чтобы понять, какую метку мы предсказали для очередного изображения, можно просто взять максимальное значение из этих результатов. В TensorFlow это выражается как tf.argmax, функция, выдающая позицию максимального элемента в тензоре по заданной оси. Для того чтобы понять, верно ли мы предсказали метку, достаточно просто сравнить между собой\n",
    "**tf.argmax(y, 1)** и **tf.argmax(y_, 1)**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Это дает на выходе список булевых значений, показывающих, верно или неверно предсказан результат; итоговой точностью может быть, например, среднее значение соответствующего вектора:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Осталось только вычислить его и вывести:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 0.9125\n"
     ]
    }
   ],
   "source": [
    "print(\"Точность: %s\" %\n",
    "      sess.run(accuracy, feed_dict={x: mnist.test.images, y_: mnist.test.labels}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "=====================================================================================\n",
    "\n",
    "Итак, первое знакомство с TensorFlow прошло удачно, но в нашем примере не было никаких глубоких сетей, да и вообще нейронных-то сетей по сути нет: мы просто обучили логистическую регрессию.  \n",
    "Давайте теперь немного усложним модель и посмотрим, что из этого получится.  \n",
    "\n",
    "Во-первых, чтобы наша модель вообще имела право носить гордое название нейронной сети, нужно добавить в нее скрытый слой, назовём его «слой ReLU». В качестве функции активации возьмем все тот же ReLU. Нужно инициализировать веса и свободный член для этого слоя; будем в этом примере использовать 100 нейронов на скрытом слое:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_relu = tf.Variable(tf.truncated_normal([784, 100], stddev=0.1))\n",
    "b_relu = tf.Variable(tf.truncated_normal([100], stddev=0.1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "На этот раз мы инициализируем переменные не нулями, а небольшими случайными значениями. Функция __tf.truncated_normal__ возвращает значения, порожденные нормально распределенной случайной величиной с фиксированными математическим ожиданием (в нашем примере мы оставили значение 0, задающееся по умолчанию) и дисперсией (в нашем примере __stddev=0.l__); однако при этом значения, вышедшие за пределы интервала в **±2σ** от среднего, выбираются заново, то есть распределение обрезается так, чтобы полностью запретить большие выбросы. Инициализация нулями в данном случае была бы совсем бессмысленной, потому что **ReLU(0) = 0**, а значит, при инициализации нулями градиенты совсем не распространялись бы по сети. В нашем же случае примерно половина весов окажется отрицательной, и соответствующие нейроны не будут активироваться вовсе.  \n",
    "\n",
    "Итак, хотя теория градиентного спуска от этого не меняется никак, на практике хорошая инициализация весов очень важна, чтобы попасть в разумную область значений целевой функции и не застрять в плохом локальном максимуме.   \n",
    "\n",
    "А сейчас общий вид скрытого слоя получается таким:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = tf.nn.relu(tf.matmul(x, W_relu) + b_relu)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Применение **tf.nn.relu** тоже будет покомпонентным: фактически мы просто применили функцию **ReLU** к вектору **tf.matmul(x, W_relu) + b_relu**.  \n",
    "\n",
    "В этой нейронной сети будет очень полезен слой ***дропаута***. Дропаут — это слой, который выбрасывает (обнуляет) выходы некоторых нейронов, выбираемых случайно и заново для каждого обучающего примера. Все, что нам сейчас нужно задать — это вероятность их выбрасывания; для этого мы сначала создадим заглушку:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_probability = tf.placeholder(tf.float32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "А дальше TensorFlow, как всегда, всё делает за нас, достаточно только правильно попросить:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "h_drop = tf.nn.dropout(h, keep_probability)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь нейроны скрытого слоя будут участвовать в вычислениях с вероятностью **keep_probabllity**, а с вероятностью **1-keep_probability** их выход будет обнулен, и они не будут ни участвовать в предсказании для этого примера, ни обучаться на нем. Поскольку размер внутреннего слоя отличается от входного, нам придется немного поменять параметры внешнего слоя и, кроме того, переписать заключительный softmax-слой:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = tf.Variable(tf.zeros([100, 10]))\n",
    "b = tf.Variable(tf.zeros([10]))\n",
    "y = tf.nn.softmax(tf.matmul(h_drop, W) + b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "При этом наша функция потерь ***cross_entropy*** и оптимизатор ***train_step*** не требуют никаких изменений. А вот вызывать ***sess.run*** нужно с новым параметром ***keep_probability***. Поэтому цикл обучения немного изменился:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(2000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_probability: 0.5})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Если сделать 2000 шагов обучения нашей новой сети, мы получим:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 0.9236\n"
     ]
    }
   ],
   "source": [
    "print(\"Точность: %s\" % sess.run(accuracy, feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_probability: 1.}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "==========================================================================================\n",
    "\n",
    "Чтобы избежать возможного переполнения при слишком большом количестве циклов обучения, создатели TensorFlow знали об этих трудностях и специально подготовили функцию, которая обходит все подводные камни. Давайте использовать ее вместо самописных функций. Зададим промежуточный тензор:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "logit = tf.matmul(h_drop, W) + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Теперь мы не будем сами применять softmax или считать перекрестную энтропию, а воспользуемся готовой функцией:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits_v2(logits=logit, labels=y_))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Больше ничего менять не надо; давайте снова запустим код, теперь уже сделав 10000 шагов обучения, и посмотрим на результат:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Точность: 0.9241\n"
     ]
    }
   ],
   "source": [
    "for i in range(10000):\n",
    "    batch_xs, batch_ys = mnist.train.next_batch(100)\n",
    "    sess.run(train_step, feed_dict={x: batch_xs, y_: batch_ys, keep_probability: 0.5})\n",
    "    \n",
    "print(\"Точность: %s\" % sess.run(accuracy, feed_dict={\n",
    "    x: mnist.test.images, y_: mnist.test.labels, keep_probability: 1.}))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
